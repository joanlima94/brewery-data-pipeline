# Brewery DB Data Pipeline

This project implements a **medallion architecture** (Bronze â†’ Silver â†’ Gold) to ingest, transform, and serve data from the [Open Brewery DB API](https://www.openbrewerydb.org/documentation). The pipeline is orchestrated with **Apache Airflow**, containerized with **Docker**, deployable on **Kubernetes**.

---

## ğŸŒ Data Source

- **API**: `https://api.openbrewerydb.org/v1/breweries`
- **Metadata endpoint**: `https://api.openbrewerydb.org/v1/breweries/meta`
- **Pagination**: Up to 200 records per page
- **Total records**: ~10,000 breweries (as of 2025)

---

# Brewery DB Data Pipeline

This project implements a **medallion architecture** (Bronze â†’ Silver â†’ Gold) to ingest, transform, and serve data from the [Open Brewery DB API](https://www.openbrewerydb.org/documentation). The pipeline is orchestrated with **Apache Airflow**, containerized with **Docker**, deployable on **Kubernetes**, and monitored via **Grafana**.

Default credentials to access Airflow UI:
```bash
_AIRFLOW_WWW_USER_USERNAME: airflow
_AIRFLOW_WWW_USER_PASSWORD: airflow
```
---

## ğŸŒ Data Source

- **API**: `https://api.openbrewerydb.org/v1/breweries`
- **Metadata endpoint**: `https://api.openbrewerydb.org/v1/breweries/meta`
- **Pagination**: Up to 200 records per page
- **Total records**: ~10,000 breweries (as of 2025)

---


### Layers

- **Bronze**: Raw JSON responses from the API, partitioned by ingestion date.
- **Silver**: Cleaned, deduplicated, strongly-typed Parquet files with schema enforcement and a `processed_at` timestamp.
- **Gold**: (Future) Business-ready aggregations (e.g., count by state, brewery type distribution).

All data is stored in a **PersistentVolume** (`/tmp/brewery-data` on your host) for local development.

---

## ğŸ§ª Key Features

- âœ… **Idempotent daily ingestion** (partitioned by date)
- âœ… **PySpark-based transformation** (scalable, schema-on-read + schema enforcement)
- âœ… **Snappy-compressed Parquet** output for efficient storage and query performance
- âœ… **Unit tests** for ingestion and transformation logic
- âœ… **Airflow DAG** using `KubernetesPodOperator` for task isolation
---

## ğŸ“¦ Project Structure

```bash
brewery-data-pipeline/
â”œâ”€â”€ airflow/                     # Airflow DAGs + docker-compose
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ brewery_pipeline.py  # Main DAG
â”‚   â””â”€â”€ docker-compose.yaml      # Local Airflow + Postgres + Redis
â”œâ”€â”€ etl/                         # Data pipeline code
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ api_to_bronze.py     # Ingests API â†’ JSON (Bronze)
â”‚   â”‚   â”œâ”€â”€ transform_bronze_to_silver.py
â”‚   â”‚   â””â”€â”€ aggregated_silver_to_gold.py
â”‚   â””â”€â”€ tests/                   # Unit tests
â”œâ”€â”€ k8s/                         # Kubernetes manifests
â”‚   â”œâ”€â”€ pv-pvc.yaml              # PersistentVolume for data
â”‚   â””â”€â”€ airflow-rbac.yaml        # RBAC for Airflow pods
â”œâ”€â”€ Dockerfile                   # Builds pipeline image
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ README.md
``` 

## ğŸ“¦ Automate Setup with MakeFile

```bash
	make all             - Setup all (build, infra, Airflow, DAGs)
	make build-image     - build Docker image
	make apply-k8s       - Apply PV/PVC/RBAC in Kubernetes cluster
	make start-airflow   - Start Airflow, Postgres, Grafana
	make copy-dags       - Update DAG in Airflow
	make test            - Run unit and integration tests
	make status          - Show status of cluster and containers
	make stop            - Stop everything
```

## ğŸŒ Deploy on Cloud

Following the instructions for setup on Google Cloud Plataform

- **IAM permissions**: roles/container.admin, roles/storage.admin, roles/composer.admin and roles/logging.admin

Create a GKE Cluster
```bash
gcloud config set project YOUR_PROJECT_ID

gcloud container clusters create brewery-cluster \
  --zone us-central1-a \
  --num-nodes 3 \
  --machine-type e2-medium \
  --enable-autoscaling --min-nodes 1 --max-nodes 5
```

Create a Cloud Storage Bucket
```bash
gsutil mb -l us-central1 gs://brewery-data-YOUR_PROJECT_ID

# This for replace the local PersistentVolume
```

Update Your Code for Cloud Storage

```bash
# Instead of:
bronze_path = Path(data_root) / f"medallion/bronze/breweries/{execution_date}"
silver_path = Path(data_root) / f"medallion/silver/breweries/{execution_date}"
gold_path = Path(data_root) / f"medallion/gold/breweries_agg/{execution_date}"

# Use:
bronze_path = "gs://brewery-data-YOUR_PROJECT_ID/medallion/bronze/..."
silver_path = "gs://brewery-data-YOUR_PROJECT_ID/medallion/silver/..."
gold_path = "gs://brewery-data-YOUR_PROJECT_ID/medallion/gold/..."
```

Build and Push Docker Image to Artifact Registry

```bash
gcloud auth configure-docker us-central1-docker.pkg.dev

docker build -t brewery-pipeline:latest .

docker tag brewery-pipeline:latest \
  us-central1-docker.pkg.dev/YOUR_PROJECT_ID/brewery-repo/brewery-pipeline:latest
docker push us-central1-docker.pkg.dev/YOUR_PROJECT_ID/brewery-repo/brewery-pipeline:latest
```

Deploy Airflow with Composer Environment (Recommended)
```bash
gcloud composer environments create brewery-composer \
  --location us-central1 \
  --zone us-central1-a \
  --image-version composer-3.1.0-airflow-2.10.0
```

Configure Monitoring (Optional)

```bash
Use Cloud Monitoring + Cloud Logging
```

## ğŸŒ Results

![alt text](image.png)

![alt text](image-1.png)

![alt text](image-2.png)

![alt text](image-3.png)
